
device set to:  cuda
/root/miniconda3/envs/edt/lib/python3.8/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
Traceback (most recent call last):
  File "scripts/eval_edm.py", line 235, in <module>
    test(cfg)
  File "scripts/eval_edm.py", line 150, in test
    model.load_state_dict(torch.load(eval_chk_pt_path, map_location=device))
  File "/root/miniconda3/envs/edt/lib/python3.8/site-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ElasticDecisionTransformer:
	Missing key(s) in state_dict: "transformer.0.attention.mask", "transformer.0.attention.q_net.weight", "transformer.0.attention.q_net.bias", "transformer.0.attention.k_net.weight", "transformer.0.attention.k_net.bias", "transformer.0.attention.v_net.weight", "transformer.0.attention.v_net.bias", "transformer.0.attention.proj_net.weight", "transformer.0.attention.proj_net.bias", "transformer.0.mlp.0.weight", "transformer.0.mlp.0.bias", "transformer.0.mlp.2.weight", "transformer.0.mlp.2.bias", "transformer.0.ln1.weight", "transformer.0.ln1.bias", "transformer.0.ln2.weight", "transformer.0.ln2.bias", "transformer.1.attention.mask", "transformer.1.attention.q_net.weight", "transformer.1.attention.q_net.bias", "transformer.1.attention.k_net.weight", "transformer.1.attention.k_net.bias", "transformer.1.attention.v_net.weight", "transformer.1.attention.v_net.bias", "transformer.1.attention.proj_net.weight", "transformer.1.attention.proj_net.bias", "transformer.1.mlp.0.weight", "transformer.1.mlp.0.bias", "transformer.1.mlp.2.weight", "transformer.1.mlp.2.bias", "transformer.1.ln1.weight", "transformer.1.ln1.bias", "transformer.1.ln2.weight", "transformer.1.ln2.bias", "transformer.2.attention.mask", "transformer.2.attention.q_net.weight", "transformer.2.attention.q_net.bias", "transformer.2.attention.k_net.weight", "transformer.2.attention.k_net.bias", "transformer.2.attention.v_net.weight", "transformer.2.attention.v_net.bias", "transformer.2.attention.proj_net.weight", "transformer.2.attention.proj_net.bias", "transformer.2.mlp.0.weight", "transformer.2.mlp.0.bias", "transformer.2.mlp.2.weight", "transformer.2.mlp.2.bias", "transformer.2.ln1.weight", "transformer.2.ln1.bias", "transformer.2.ln2.weight", "transformer.2.ln2.bias", "transformer.3.attention.mask", "transformer.3.attention.q_net.weight", "transformer.3.attention.q_net.bias", "transformer.3.attention.k_net.weight", "transformer.3.attention.k_net.bias", "transformer.3.attention.v_net.weight", "transformer.3.attention.v_net.bias", "transformer.3.attention.proj_net.weight", "transformer.3.attention.proj_net.bias", "transformer.3.mlp.0.weight", "transformer.3.mlp.0.bias", "transformer.3.mlp.2.weight", "transformer.3.mlp.2.bias", "transformer.3.ln1.weight", "transformer.3.ln1.bias", "transformer.3.ln2.weight", "transformer.3.ln2.bias".
	Unexpected key(s) in state_dict: "mamba.0.norm_mamba.weight", "mamba.0.norm_mamba.bias", "mamba.0.mamba.A_log", "mamba.0.mamba.D", "mamba.0.mamba.in_proj.weight", "mamba.0.mamba.conv1d.weight", "mamba.0.mamba.conv1d.bias", "mamba.0.mamba.x_proj.weight", "mamba.0.mamba.dt_proj.weight", "mamba.0.mamba.dt_proj.bias", "mamba.0.mamba.out_proj.weight", "mamba.0.ln_2.weight", "mamba.0.ln_2.bias", "mamba.0.mlp_channels.0.weight", "mamba.0.mlp_channels.0.bias", "mamba.0.mlp_channels.2.weight", "mamba.0.mlp_channels.2.bias", "mamba.1.norm_mamba.weight", "mamba.1.norm_mamba.bias", "mamba.1.mamba.A_log", "mamba.1.mamba.D", "mamba.1.mamba.in_proj.weight", "mamba.1.mamba.conv1d.weight", "mamba.1.mamba.conv1d.bias", "mamba.1.mamba.x_proj.weight", "mamba.1.mamba.dt_proj.weight", "mamba.1.mamba.dt_proj.bias", "mamba.1.mamba.out_proj.weight", "mamba.1.ln_2.weight", "mamba.1.ln_2.bias", "mamba.1.mlp_channels.0.weight", "mamba.1.mlp_channels.0.bias", "mamba.1.mlp_channels.2.weight", "mamba.1.mlp_channels.2.bias", "mamba.2.norm_mamba.weight", "mamba.2.norm_mamba.bias", "mamba.2.mamba.A_log", "mamba.2.mamba.D", "mamba.2.mamba.in_proj.weight", "mamba.2.mamba.conv1d.weight", "mamba.2.mamba.conv1d.bias", "mamba.2.mamba.x_proj.weight", "mamba.2.mamba.dt_proj.weight", "mamba.2.mamba.dt_proj.bias", "mamba.2.mamba.out_proj.weight", "mamba.2.ln_2.weight", "mamba.2.ln_2.bias", "mamba.2.mlp_channels.0.weight", "mamba.2.mlp_channels.0.bias", "mamba.2.mlp_channels.2.weight", "mamba.2.mlp_channels.2.bias", "mamba.3.norm_mamba.weight", "mamba.3.norm_mamba.bias", "mamba.3.mamba.A_log", "mamba.3.mamba.D", "mamba.3.mamba.in_proj.weight", "mamba.3.mamba.conv1d.weight", "mamba.3.mamba.conv1d.bias", "mamba.3.mamba.x_proj.weight", "mamba.3.mamba.dt_proj.weight", "mamba.3.mamba.dt_proj.bias", "mamba.3.mamba.out_proj.weight", "mamba.3.ln_2.weight", "mamba.3.ln_2.bias", "mamba.3.mlp_channels.0.weight", "mamba.3.mlp_channels.0.bias", "mamba.3.mlp_channels.2.weight", "mamba.3.mlp_channels.2.bias".